# üìö Guide d'utilisation - AnimeSamaScraper (Linux)

## üéØ R√©sum√© des am√©liorations

Les scripts ont √©t√© optimis√©s pour un d√©ploiement et une maintenance 100% Linux avec :
- üìñ Comptage pr√©cis des pages pour chaque chapitre
- üìä Statistiques compl√®tes (mangas, chapitres, pages)
- üîÑ Automatisation compl√®te du processus avec systemd
- üíæ Stockage structur√© en base de donn√©es MongoDB
- üìà Suivi des m√©triques de contenu
- üõ†Ô∏è **Scripts de maintenance Linux automatis√©s**
- üîç **Surveillance continue du service**
- üìß **Alertes et notifications (extensible)**

### üöÄ Workflow complet de mise √† jour (Linux)
1. **D√©veloppement local** ‚Üí Test avec `python test_updated_scripts.py`
2. **Commit et push** ‚Üí `git add . && git commit -m "..." && git push`
3. **Mise √† jour production** ‚Üí `./update_service.sh`
4. **Surveillance** ‚Üí `./monitor_service.sh --detailed`

## üìÅ Scripts disponibles

### 1. `main.py` (Script principal)
- ‚úÖ **Fonctionnalit√© principale** : Scraping complet des mangas avec comptage des pages
- ‚úÖ **Nouvelle fonction** : `process_all_steps_in_order()` - Ex√©cute les 4 √©tapes automatiquement
- ‚úÖ **Am√©lioration** : Parse correctement les fichiers `episodes.js` au format JavaScript

### 2. `add_to_db.py` (Ajout en base de donn√©es)
- ‚úÖ **Fonctionnalit√© principale** : Insertion des donn√©es scrap√©es dans MongoDB
- ‚úÖ **Nouvelles donn√©es** : Stockage du `page_count`, `scan_id`, `episodes_url` pour chaque chapitre
- ‚úÖ **Nouvelles statistiques** : Comptage total des pages, moyenne de pages par chapitre
- ‚úÖ **Am√©lioration** : Affichage enrichi des recherches avec nombre de chapitres et pages

### 3. `daily_scraper.py` (Scraping automatique)
- ‚úÖ **Fonctionnalit√© principale** : Scraping quotidien automatis√© avec scheduler
- ‚úÖ **Am√©lioration** : Utilise maintenant le processus complet (4 √©tapes) incluant le scraping des chapitres
- ‚úÖ **Nouvelles options** : `--test-db` pour tester la connexion MongoDB
- ‚úÖ **Logging am√©lior√©** : Statistiques d√©taill√©es des chapitres et pages scrap√©s

### 4. `update_service.sh` (Mise √† jour automatique)
- ‚úÖ **Script de maintenance Linux** : Automatise la mise √† jour du service
- ‚úÖ **Fonctionnalit√©s** : Arr√™t/d√©marrage service, git pull, tests, v√©rifications
- ‚úÖ **S√©curis√©** : V√©rifications √† chaque √©tape, rollback possible

### 5. `monitor_service.sh` (Surveillance)
- ‚úÖ **Surveillance compl√®te** : Service, logs, espace disque, r√©seau
- ‚úÖ **Options avanc√©es** : Red√©marrage automatique, alertes, rapports d√©taill√©s
- ‚úÖ **Compatible cron** : Peut √™tre automatis√© pour surveillance continue

## üöÄ Utilisation

### Scraping manuel complet
```bash
# Ex√©cuter le script principal (mode interactif)
python main.py

# Puis suivre les √©tapes 1‚Üí2‚Üí3‚Üí4 dans le menu
```

### Scraping automatique en arri√®re-plan
```bash
# Ex√©cuter le scraping quotidien imm√©diatement
python daily_scraper.py --now

# Tester la connexion √† la base de donn√©es
python daily_scraper.py --test-db
```

### Ajout manuel en base
```bash
# Ajouter les donn√©es du fichier JSON dans MongoDB
python add_to_db.py

# Le script utilise automatiquement anime_data.json
```

## üóÑÔ∏è Structure des donn√©es

### Collection `anime_list`
- M√©tadonn√©es des mangas
- **Nouveau** : `total_pages` - Total de pages pour tous les chapitres
- **Nouveau** : `scan_chapters` - Donn√©es compl√®tes des scans

### Collection `chapters`
- Chapitres individuels
- **Nouveau** : `page_count` - Nombre de pages du chapitre
- **Nouveau** : `scan_id` - ID du scan source
- **Nouveau** : `episodes_url` - URL du fichier episodes.js

## üìà Statistiques disponibles

Le script `add_to_db.py` affiche maintenant :
- Nombre total de mangas et chapitres
- **Nouveau** : Nombre total de pages index√©es
- **Nouveau** : Moyenne de pages par chapitre
- Top 5 des mangas avec le plus de chapitres (avec nombre de pages)

## üîß D√©pendances requises

```bash
# Installation des d√©pendances
pip install -r requirements.txt

# Ou manuellement :
pip install pymongo python-dotenv schedule requests beautifulsoup4
```

## üìù Configuration MongoDB

Cr√©er un fichier `.env` avec :
```env
MONGO_URL=mongodb+srv://username:password@cluster.mongodb.net/database_name
```

## ‚ö° Tests

Pour v√©rifier que tout fonctionne :
```bash
python test_updated_scripts.py
```

## üïê Planification automatique

### Linux (Service systemd)
```bash
# Installer le service
sudo cp anime-sama-scraper.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable anime-sama-scraper
sudo systemctl start anime-sama-scraper

# V√©rifier le statut
sudo systemctl status anime-sama-scraper
```

## üîÑ Mise √† jour du service apr√®s modifications

Le script `update_service.sh` automatise compl√®tement la mise √† jour :

```bash
# Mise √† jour automatique (recommand√©e)
./update_service.sh

# Le script effectue automatiquement :
# 1. Arr√™t du service systemd
# 2. Mise √† jour du code (git pull)
# 3. Installation des d√©pendances
# 4. Tests de validation
# 5. Red√©marrage du service
```

### V√©rification apr√®s mise √† jour
```bash
# V√©rifier le statut du service
sudo systemctl status anime-sama-scraper

# Consulter les logs pour v√©rifier le bon fonctionnement
tail -f logs/anime_sama_scraper_$(date +%Y%m%d).log

# V√©rifier les logs systemd
journalctl -u anime-sama-scraper -f
```

### üìã Bonnes pratiques pour les mises √† jour

1. **Sauvegarde avant mise √† jour** :
   ```bash
   # Sauvegarder la base de donn√©es
   mongodump --uri="your_mongo_url" --out=backup_before_update
   
   # Sauvegarder les logs
   cp -r logs logs_backup_$(date +%Y%m%d)
   ```

2. **Test en local avant d√©ploiement** :
   ```bash
   # Tester le script en mode manuel
   python daily_scraper.py --now
   
   # V√©rifier les nouvelles fonctionnalit√©s
   python test_updated_scripts.py
   ```

3. **Surveillance apr√®s mise √† jour** :
   ```bash
   # Surveiller les logs en temps r√©el
   tail -f logs/anime_sama_scraper_*.log
   ```

## üîç Surveillance et maintenance du service

### Script de surveillance Linux
```bash
# V√©rification standard
./monitor_service.sh

# V√©rification d√©taill√©e avec tous les logs
./monitor_service.sh --detailed

# Surveillance avec red√©marrage automatique si n√©cessaire
./monitor_service.sh --restart

# Surveillance compl√®te avec alertes
./monitor_service.sh --detailed --restart --email
```

### Commandes de surveillance essentielles
```bash
# V√©rifier le statut du service
sudo systemctl status anime-sama-scraper

# Surveiller les logs en temps r√©el
tail -f logs/anime_sama_scraper_$(date +%Y%m%d).log

# V√©rifier les derni√®res ex√©cutions
grep "D√âBUT DU PROCESSUS" logs/anime_sama_scraper_*.log | tail -5

# Voir les erreurs r√©centes
grep "ERROR" logs/anime_sama_scraper_*.log | tail -10

# V√©rifier les logs systemd
journalctl -u anime-sama-scraper --since "1 hour ago"
```

### V√©rifications de sant√© p√©riodiques
```bash
# V√©rifier l'espace disque
df -h

# V√©rifier l'usage m√©moire
free -h

# V√©rifier les processus Python actifs
ps aux | grep python | grep -E "(daily_scraper|main\.py)"

# V√©rifier la connectivit√© vers anime-sama.fr
ping -c 3 anime-sama.fr
```

### üîÑ Automatisation de la surveillance

#### Ajouter une t√¢che cron pour surveillance automatique
```bash
# √âditer le crontab
crontab -e

# Ajouter cette ligne pour v√©rifier toutes les heures
0 * * * * /path/to/AnimeSamaScrapper/monitor_service.sh --restart >> /var/log/anime_sama_monitor.log 2>&1

# Ou surveillance compl√®te 4 fois par jour
0 6,12,18,0 * * * /path/to/AnimeSamaScrapper/monitor_service.sh --detailed --restart --email >> /var/log/anime_sama_monitor.log 2>&1
```

#### Script de nettoyage automatique des logs
```bash
# Cr√©er un script de nettoyage
cat > cleanup_logs.sh << 'EOF'
#!/bin/bash
# Nettoyage automatique des logs anciens
find logs/ -name "*.log" -mtime +30 -exec gzip {} \;
find logs/ -name "*.log.gz" -mtime +90 -delete
journalctl --vacuum-time=30d
echo "Nettoyage des logs termin√©: $(date)"
EOF

chmod +x cleanup_logs.sh

# Ajouter au cron (une fois par semaine)
# 0 2 * * 0 /path/to/AnimeSamaScrapper/cleanup_logs.sh >> /var/log/cleanup.log 2>&1
```

## üõ†Ô∏è Processus complet du scraping

Le script effectue maintenant les √©tapes suivantes :
1. ‚úÖ Scrape la liste des mangas depuis anime-sama.fr
2. ‚úÖ Parse les m√©tadonn√©es de chaque manga
3. ‚úÖ R√©cup√®re les informations des chapitres via episodes.js
4. ‚úÖ Compte les pages de chaque chapitre
5. ‚úÖ Met √† jour la base de donn√©es MongoDB
6. ‚úÖ G√©n√®re des logs d√©taill√©s avec statistiques

## üìä M√©triques et rapports

### Logs d√©taill√©s disponibles
- Nombre de mangas trait√©s
- Nombre de chapitres scrap√©s
- Total de pages comptabilis√©es
- Dur√©e du processus
- Erreurs et avertissements
- √âtat de la base de donn√©es

### Commandes de statistiques
```bash
# Statistiques g√©n√©rales du scraping
python add_to_db.py

# Logs des derni√®res ex√©cutions
grep "STATISTIQUES FINALES" logs/anime_sama_scraper_*.log | tail -5

# Performance du scraping
grep "Dur√©e totale" logs/anime_sama_scraper_*.log | tail -10
```

## üö® D√©pannage

### Probl√®mes courants et solutions

#### 1. Service qui ne d√©marre pas
```bash
# V√©rifier les logs systemd
journalctl -u anime-sama-scraper --no-pager

# V√©rifier la configuration du service
sudo systemctl cat anime-sama-scraper

# Tester manuellement
python daily_scraper.py --now
```

#### 2. Erreurs de connexion MongoDB
```bash
# Tester la connexion
python daily_scraper.py --test-db

# V√©rifier les variables d'environnement
cat .env

# Tester avec mongosh/mongo client
mongosh "$MONGO_URL"
```

#### 3. Erreurs de scraping
```bash
# V√©rifier la connectivit√©
curl -I https://anime-sama.fr

# Tester le scraping manuel
python main.py

# Analyser les logs d'erreur
grep "ERROR" logs/anime_sama_scraper_*.log | tail -20
```

#### 4. Performance lente
```bash
# V√©rifier l'usage des ressources
htop

# Analyser les temps de r√©ponse
grep "Dur√©e" logs/anime_sama_scraper_*.log | tail -10

# V√©rifier l'espace disque
df -h
```

## üìà R√©sum√© des am√©liorations

Avec ces am√©liorations, le syst√®me AnimeSamaScraper est maintenant :
- üêß **100% Linux** - D√©ploiement et maintenance natifs
- üìñ **Complet** - Comptage pr√©cis des pages pour chaque chapitre
- üìä **Statistiques riches** - M√©triques compl√®tes (mangas, chapitres, pages)
- üîÑ **Automatis√©** - Service systemd + scripts de maintenance
- üíæ **Robuste** - Stockage structur√© MongoDB avec gestion d'erreurs
- üìà **Monitor√©** - Surveillance continue et alertes automatiques
- üõ†Ô∏è **Maintenable** - Scripts de mise √† jour et diagnostic int√©gr√©s

Le syst√®me est maintenant **pr√™t pour la production** avec une maintenance automatis√©e ! ‚úÖ
